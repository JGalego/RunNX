theory OperatorSpecs
  use int.Int
  use real.Real
  use array.Array

  (* Type definitions for tensors *)
  type shape = array int
  type tensor_data = array real
  
  type tensor = {
    shape: shape;
    data: tensor_data;
    size: int;
  }

  (* Predicates for tensor validity *)
  predicate valid_tensor (t: tensor) =
    t.size >= 0 /\
    length t.shape >= 1 /\
    length t.data = t.size

  predicate same_shape (t1 t2: tensor) =
    length t1.shape = length t2.shape /\
    forall i. 0 <= i < length t1.shape -> t1.shape[i] = t2.shape[i]

  predicate broadcastable (t1 t2: tensor) =
    (* Simplified broadcasting: either same shape or one is scalar *)
    same_shape t1 t2 \/ t1.size = 1 \/ t2.size = 1

  (* Addition operator specification *)
  function add_element (a b: real) : real = a + b

  predicate add_spec (input1 input2 output: tensor) =
    valid_tensor input1 /\
    valid_tensor input2 /\
    valid_tensor output /\
    broadcastable input1 input2 /\
    same_shape input1 output /\
    (input1.size = input2.size -> same_shape input2 output) /\
    forall i. 0 <= i < output.size ->
      output.data[i] = add_element input1.data[i] input2.data[i]

  (* Properties of addition *)
  lemma add_commutativity:
    forall t1 t2 result1 result2: tensor.
    add_spec t1 t2 result1 /\ add_spec t2 t1 result2 ->
    forall i. 0 <= i < result1.size -> result1.data[i] = result2.data[i]

  lemma add_associativity:
    forall a b c ab_c a_bc: tensor.
    add_spec a b ab_c /\ add_spec ab_c c result1 /\
    add_spec b c a_bc /\ add_spec a a_bc result2 ->
    forall i. 0 <= i < result1.size -> result1.data[i] = result2.data[i]

  (* YOLO Operator Specifications *)

  (* Softmax operator specification *)
  function exp_real (x: real) : real
  function sum_array (arr: tensor_data) (size: int) : real

  predicate softmax_spec (input output: tensor) =
    valid_tensor input /\
    valid_tensor output /\
    same_shape input output /\
    (* Numerical stability: subtract max before exponential *)
    let max_val = max_array input.data input.size in
    (* Sum of exponentials after max subtraction *)
    let exp_sum = sum_exp_shifted input.data input.size max_val in
    exp_sum > 0.0 /\
    (* Each output element is normalized exponential *)
    forall i. 0 <= i < output.size ->
      output.data[i] = exp_real (input.data[i] - max_val) / exp_sum /\
      0.0 < output.data[i] < 1.0

  (* Helper functions for softmax *)
  function max_array (arr: tensor_data) (size: int) : real
  function sum_exp_shifted (arr: tensor_data) (size: int) (shift: real) : real

  (* Softmax probability distribution property *)
  lemma softmax_probability_sum:
    forall input output: tensor.
    softmax_spec input output ->
    sum_array output.data output.size = 1.0

  (* Softmax numerical stability property *)
  lemma softmax_stability:
    forall input shifted_input output1 output2: tensor, shift: real.
    (forall i. 0 <= i < input.size -> shifted_input.data[i] = input.data[i] - shift) /\
    same_shape input shifted_input /\
    softmax_spec input output1 /\
    softmax_spec shifted_input output2 ->
    forall i. 0 <= i < output1.size -> output1.data[i] = output2.data[i]

  (* Concatenation operator specification *)
  predicate concat_spec (input1 input2 output: tensor) (axis: int) =
    valid_tensor input1 /\
    valid_tensor input2 /\
    valid_tensor output /\
    0 <= axis < length input1.shape /\
    length input1.shape = length input2.shape /\
    length output.shape = length input1.shape /\
    (* All dimensions except axis must match *)
    (forall i. 0 <= i < length input1.shape /\ i <> axis ->
      input1.shape[i] = input2.shape[i] /\
      output.shape[i] = input1.shape[i]) /\
    (* Output dimension at axis is sum of input dimensions *)
    output.shape[axis] = input1.shape[axis] + input2.shape[axis]

  (* Slice operator specification *)
  predicate slice_spec (input output: tensor) (starts ends: array int) =
    valid_tensor input /\
    valid_tensor output /\
    length starts = length ends /\
    length starts <= length input.shape /\
    (* Valid slice ranges *)
    (forall i. 0 <= i < length starts ->
      0 <= starts[i] < ends[i] <= input.shape[i]) /\
    (* Output shape matches slice dimensions *)
    (forall i. 0 <= i < length starts ->
      output.shape[i] = ends[i] - starts[i])

  (* Upsample operator specification *)
  predicate upsample_spec (input output: tensor) (scale_factors: array real) =
    valid_tensor input /\
    valid_tensor output /\
    length scale_factors = length input.shape /\
    length output.shape = length input.shape /\
    (* Scale factors must be positive *)
    (forall i. 0 <= i < length scale_factors -> scale_factors[i] > 0.0) /\
    (* Output dimensions are scaled *)
    (forall i. 0 <= i < length input.shape ->
      output.shape[i] = real_to_int (int_to_real input.shape[i] * scale_factors[i]))

  (* MaxPool operator specification *)
  predicate maxpool_spec (input output: tensor) (kernel_size strides padding: array int) =
    valid_tensor input /\
    valid_tensor output /\
    length kernel_size = length strides /\
    length strides = length padding /\
    (* Kernel size must be positive *)
    (forall i. 0 <= i < length kernel_size -> kernel_size[i] > 0) /\
    (* Strides must be positive *)
    (forall i. 0 <= i < length strides -> strides[i] > 0) /\
    (* Output contains maximum values from pooling windows *)
    forall i. 0 <= i < output.size ->
      exists j. 0 <= j < input.size /\
      input.data[j] <= output.data[i]

  (* Non-Maximum Suppression specification *)
  predicate nms_spec (boxes scores: tensor) (iou_threshold score_threshold: real) (indices: array int) =
    valid_tensor boxes /\
    valid_tensor scores /\
    length boxes.shape = 2 /\
    boxes.shape[1] = 4 /\ (* [N, 4] format *)
    length scores.shape = 1 /\
    scores.shape[0] = boxes.shape[0] /\
    0.0 <= iou_threshold <= 1.0 /\
    score_threshold >= 0.0 /\
    (* Output indices are sorted by descending scores *)
    (forall i j. 0 <= i < j < length indices ->
      scores.data[indices[i]] >= scores.data[indices[j]]) /\
    (* All selected boxes have IoU below threshold *)
    (forall i j. 0 <= i < j < length indices ->
      iou_boxes boxes indices[i] indices[j] <= iou_threshold)

  (* Helper function for IoU calculation *)
  function iou_boxes (boxes: tensor) (idx1 idx2: int) : real

  (* Utility functions *)
  function int_to_real (x: int) : real
  function real_to_int (x: real) : int
    (exists ab. add_spec a b ab /\ add_spec ab c ab_c) /\
    (exists bc. add_spec b c bc /\ add_spec a bc a_bc) ->
    forall i. 0 <= i < ab_c.size -> ab_c.data[i] = a_bc.data[i]

  (* Multiplication operator specification *)
  function mul_element (a b: real) : real = a * b

  predicate mul_spec (input1 input2 output: tensor) =
    valid_tensor input1 /\
    valid_tensor input2 /\
    valid_tensor output /\
    broadcastable input1 input2 /\
    same_shape input1 output /\
    (input1.size = input2.size -> same_shape input2 output) /\
    forall i. 0 <= i < output.size ->
      output.data[i] = mul_element input1.data[i] input2.data[i]

  (* Properties of multiplication *)
  lemma mul_commutativity:
    forall t1 t2 result1 result2: tensor.
    mul_spec t1 t2 result1 /\ mul_spec t2 t1 result2 ->
    forall i. 0 <= i < result1.size -> result1.data[i] = result2.data[i]

  lemma mul_associativity:
    forall a b c ab_c a_bc: tensor.
    (exists ab. mul_spec a b ab /\ mul_spec ab c ab_c) /\
    (exists bc. mul_spec b c bc /\ mul_spec a bc a_bc) ->
    forall i. 0 <= i < ab_c.size -> ab_c.data[i] = a_bc.data[i]

  (* Matrix multiplication specification *)
  predicate matrix_compatible (t1 t2: tensor) =
    length t1.shape = 2 /\
    length t2.shape = 2 /\
    t1.shape[1] = t2.shape[0]

  function matmul_element (t1 t2: tensor) (i j: int) : real =
    let rec sum_product (k: int) : real =
      if k < 0 then 0.0
      else 
        let idx1 = i * t1.shape[1] + k in
        let idx2 = k * t2.shape[1] + j in
        t1.data[idx1] * t2.data[idx2] + sum_product (k - 1)
    in
    sum_product (t1.shape[1] - 1)

  predicate matmul_spec (input1 input2 output: tensor) =
    valid_tensor input1 /\
    valid_tensor input2 /\
    valid_tensor output /\
    matrix_compatible input1 input2 /\
    length output.shape = 2 /\
    output.shape[0] = input1.shape[0] /\
    output.shape[1] = input2.shape[1] /\
    output.size = output.shape[0] * output.shape[1] /\
    forall i j. 0 <= i < output.shape[0] /\ 0 <= j < output.shape[1] ->
      let idx = i * output.shape[1] + j in
      output.data[idx] = matmul_element input1 input2 i j

  (* ReLU activation function specification *)
  function relu_element (x: real) : real =
    if x >= 0.0 then x else 0.0

  predicate relu_spec (input output: tensor) =
    valid_tensor input /\
    valid_tensor output /\
    same_shape input output /\
    forall i. 0 <= i < output.size ->
      output.data[i] = relu_element input.data[i]

  (* Properties of ReLU *)
  lemma relu_non_negative:
    forall input output: tensor.
    relu_spec input output ->
    forall i. 0 <= i < output.size -> output.data[i] >= 0.0

  lemma relu_idempotent:
    forall input output1 output2: tensor.
    relu_spec input output1 /\ relu_spec output1 output2 ->
    forall i. 0 <= i < output1.size -> output1.data[i] = output2.data[i]

  lemma relu_monotonic:
    forall input1 input2 output1 output2: tensor.
    same_shape input1 input2 /\
    relu_spec input1 output1 /\ relu_spec input2 output2 /\
    (forall i. 0 <= i < input1.size -> input1.data[i] <= input2.data[i]) ->
    forall i. 0 <= i < output1.size -> output1.data[i] <= output2.data[i]

  (* Sigmoid activation function specification *)
  function sigmoid_element (x: real) : real =
    1.0 / (1.0 + exp (-x))

  predicate sigmoid_spec (input output: tensor) =
    valid_tensor input /\
    valid_tensor output /\
    same_shape input output /\
    forall i. 0 <= i < output.size ->
      output.data[i] = sigmoid_element input.data[i]

  (* Properties of Sigmoid *)
  lemma sigmoid_bounded:
    forall input output: tensor.
    sigmoid_spec input output ->
    forall i. 0 <= i < output.size -> 
      0.0 < output.data[i] < 1.0

  lemma sigmoid_monotonic:
    forall input1 input2 output1 output2: tensor.
    same_shape input1 input2 /\
    sigmoid_spec input1 output1 /\ sigmoid_spec input2 output2 /\
    (forall i. 0 <= i < input1.size -> input1.data[i] < input2.data[i]) ->
    forall i. 0 <= i < output1.size -> output1.data[i] < output2.data[i]

  (* Transpose operation specification *)
  predicate transpose_2d_spec (input output: tensor) =
    valid_tensor input /\
    valid_tensor output /\
    length input.shape = 2 /\
    length output.shape = 2 /\
    input.shape[0] = output.shape[1] /\
    input.shape[1] = output.shape[0] /\
    input.size = output.size /\
    forall i j. 0 <= i < input.shape[0] /\ 0 <= j < input.shape[1] ->
      let input_idx = i * input.shape[1] + j in
      let output_idx = j * output.shape[0] + i in
      output.data[output_idx] = input.data[input_idx]

  (* Properties of Transpose *)
  lemma transpose_involution:
    forall input intermediate output: tensor.
    transpose_2d_spec input intermediate /\ 
    transpose_2d_spec intermediate output ->
    forall i. 0 <= i < input.size -> input.data[i] = output.data[i]

  (* Reshape operation specification *)
  predicate reshape_spec (input output: tensor) (new_shape: shape) =
    valid_tensor input /\
    valid_tensor output /\
    input.size = output.size /\
    output.shape = new_shape /\
    forall i. 0 <= i < input.size -> input.data[i] = output.data[i]

  (* Properties of Reshape *)
  lemma reshape_preserves_data:
    forall input output: tensor, new_shape: shape.
    reshape_spec input output new_shape ->
    forall i. 0 <= i < input.size -> input.data[i] = output.data[i]

end
